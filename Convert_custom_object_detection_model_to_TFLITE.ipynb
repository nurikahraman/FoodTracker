{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Convert custom object detection model to TFLITE.ipynb",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurikahraman/FoodTracker/blob/main/Convert_custom_object_detection_model_to_TFLITE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaTTQ6iSRVZx"
      },
      "source": [
        "# Convert custom object detection model to TFLITE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8OPyvLvtwmY"
      },
      "source": [
        "<table align=\"left\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/TannerGilbert/Tensorflow-Lite-Object-Detection-with-the-Tensorflow-Object-Detection-API/blob/master/Convert_custom_object_detection_model_to_TFLITE.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
        "  </a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/TannerGilbert/Tensorflow-Lite-Object-Detection-with-the-Tensorflow-Object-Detection-API/blob/master/Convert_custom_object_detection_model_to_TFLITE.ipynb\">\n",
        "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "</td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vae79rIctwmd"
      },
      "source": [
        "This notebook walks you through training a custom object detection model using the Tensorflow Object Detection API and Tensorflow 2.\n",
        "\n",
        "The notebook is split into the following parts:\n",
        "* Install the Tensorflow Object Detection API\n",
        "* Prepare data for use with the OD API\n",
        "* Write custom training configuration\n",
        "* Train detector\n",
        "* Export model inference graph\n",
        "* Test trained model\n",
        "* Convert model to Tensorflow Lite\n",
        "* Testing converted model\n",
        "\n",
        "![Microcontroller Detection](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/blob/master/doc/output.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNT9cI_ZSCla"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Installing the Tensorflow Object Detection API became a lot easier with the relase of Tensorflow 2. The following few cells are all that is needed in order to install the OD API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTBYWlnKSD78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441a1d01-e48b-4d6f-91b1-7152c2a3509e"
      },
      "source": [
        "!pip install tf-nightly -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.0/590.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.1/579.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHRp3P6448Bx"
      },
      "source": [
        "!pip install -q tflite_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpha2-F_SGBj"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmr2UdV_SHuc"
      },
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzXxTBXHSNqA"
      },
      "source": [
        "#run model builder test\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz1sd2reSTxg"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "To train a robust model, you need a lot of pictures that vary greatly from each other. You can either take the pictures yourself or you can download them from the internet.\n",
        "\n",
        "After collecting the images you need to label them. For this I recommend using [LabelImg](https://github.com/tzutalin/labelImg) - an free, open source graphical image annotation tool.\n",
        "\n",
        "![LabelImg](https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg)\n",
        "\n",
        "After labeling the images, split the data into a training and testing part and convert the xml label files to csv using the [xml_to_csv.py](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/blob/master/xml_to_csv.py) script.\n",
        "\n",
        "I uploaded my Microcontroller Detection data-set on Kaggle. The below four cells are used to download and extract the data-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8XSaOzhSX-n"
      },
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "!pip install tensorrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQqz-fsJbiQ0"
      },
      "source": [
        "# only for google colab\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"nurikahraman\"\n",
        "os.environ['KAGGLE_KEY'] = \"5cff51c1c410bcb6b041971b11fab477\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cozgoQPEbo-k"
      },
      "source": [
        "!kaggle datasets download -d tannergi/microcontroller-detection --unzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbRcqKh5b-j7"
      },
      "source": [
        "!mv \"Microcontroller Detection\" microcontroller-detection"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZPCmVzTvAj-"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/master/generate_tfrecord.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KHDdTY_cm5G"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model/master/training/labelmap.pbtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IRZfTu6di4R"
      },
      "source": [
        "!python generate_tfrecord.py --csv_input=microcontroller-detection/train_labels.csv --image_dir=microcontroller-detection/train --output_path=train.record\n",
        "!python generate_tfrecord.py --csv_input=microcontroller-detection/test_labels.csv --image_dir=microcontroller-detection/test --output_path=test.record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_W_8L24c4Sk"
      },
      "source": [
        "train_record_path = '/content/train.record'\n",
        "test_record_path = '/content/test.record'\n",
        "labelmap_path = '/content/labelmap.pbtxt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK79i98YSY8a"
      },
      "source": [
        "## Configuring training\n",
        "\n",
        "Now that the data is ready it's time to create a training configuration. The OD API supports lots of models, each with its own config file. In this notebook I'm making use of EfficientDet, but you can replace it with any model available in the [Tensorflow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axko9Jd0hEI3"
      },
      "source": [
        "batch_size = 4\n",
        "num_steps = 8000\n",
        "num_eval_steps = 1000"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RNI68K_dyzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069931fc-e3df-4b02-8100-073cd1e9caba"
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-13 11:56:31--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 173.194.79.207, 108.177.96.207, 108.177.119.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|173.194.79.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244817203 (233M) [application/x-tar]\n",
            "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_resnet50_v1_fpn 100%[===================>] 233.48M  41.1MB/s    in 6.3s    \n",
            "\n",
            "2024-03-13 11:56:37 (37.1 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [244817203/244817203]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKENdH3TfhGb"
      },
      "source": [
        "fine_tune_checkpoint = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzQ84qIQelJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b7405f-417b-4115-db6c-7e01e967fa26"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\n",
        "\n",
        "base_config_path = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-13 11:56:41--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4528 (4.4K) [text/plain]\n",
            "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config’\n",
            "\n",
            "\r          ssd_resne   0%[                    ]       0  --.-KB/s               \rssd_resnet50_v1_fpn 100%[===================>]   4.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-13 11:56:42 (43.2 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config’ saved [4528/4528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ehVTRgesxS"
      },
      "source": [
        "# edit configuration file (from https://colab.research.google.com/drive/1sLqFKVV94wm-lglFq_0kGo2ciM0kecWD)\n",
        "\n",
        "import re\n",
        "\n",
        "with open(base_config_path) as f:\n",
        "    config = f.read()\n",
        "\n",
        "with open('model_config.config', 'w') as f:\n",
        "\n",
        "  # Set labelmap path\n",
        "  config = re.sub('label_map_path: \".*?\"',\n",
        "             'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
        "\n",
        "  # Set fine_tune_checkpoint path\n",
        "  config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n",
        "\n",
        "  # Set train tf-record file path\n",
        "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")',\n",
        "                  'input_path: \"{}\"'.format(train_record_path), config)\n",
        "\n",
        "  # Set test tf-record file path\n",
        "  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")',\n",
        "                  'input_path: \"{}\"'.format(test_record_path), config)\n",
        "\n",
        "  # Set number of classes.\n",
        "  config = re.sub('num_classes: [0-9]+',\n",
        "                  'num_classes: {}'.format(4), config)\n",
        "\n",
        "  # Set batch size\n",
        "  config = re.sub('batch_size: [0-9]+',\n",
        "                  'batch_size: {}'.format(batch_size), config)\n",
        "\n",
        "  # Set training steps\n",
        "  config = re.sub('num_steps: [0-9]+',\n",
        "                  'num_steps: {}'.format(num_steps), config)\n",
        "\n",
        "  # Set fine-tune checkpoint type to detection\n",
        "  config = re.sub('fine_tune_checkpoint_type: \"classification\"',\n",
        "             'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
        "\n",
        "  f.write(config)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmtrS5dihpS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb8d7d9-dd4b-4a9c-bf11-e89f53e3125d"
      },
      "source": [
        "%cat model_config.config"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal\n",
            "# loss (a.k.a Retinanet).\n",
            "# See Lin et al, https://arxiv.org/abs/1708.02002\n",
            "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
            "# Train on TPU-8\n",
            "#\n",
            "# Achieves 34.3 mAP on COCO17 Val\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 4\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: [1.0, 2.0, 0.5]\n",
            "        scales_per_octave: 2\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 640\n",
            "        width: 640\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        depth: 256\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.0004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            scale: true,\n",
            "            decay: 0.997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "        num_layers_before_predictor: 4\n",
            "        kernel_size: 3\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_resnet50_v1_fpn_keras'\n",
            "      fpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "      }\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.0004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          decay: 0.997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.25\n",
            "          gamma: 2.0\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint_version: V2\n",
            "  fine_tune_checkpoint: \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  batch_size: 4\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  use_bfloat16: true\n",
            "  num_steps: 8000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_crop_image {\n",
            "      min_object_covered: 0.0\n",
            "      min_aspect_ratio: 0.75\n",
            "      max_aspect_ratio: 3.0\n",
            "      min_area: 0.75\n",
            "      max_area: 1.0\n",
            "      overlap_thresh: 0.0\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: .04\n",
            "          total_steps: 25000\n",
            "          warmup_learning_rate: .013333\n",
            "          warmup_steps: 2000\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/train.record\"\n",
            "  }\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  label_map_path: \"/content/labelmap.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/test.record\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRTBSsYthwxG"
      },
      "source": [
        "model_dir = 'training/'\n",
        "pipeline_config_path = 'model_config.config'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv0sbQlciKWA"
      },
      "source": [
        "## Train detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2zxx5AXiNNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c878ef66-341e-4ed1-e3b3-a2abd3e03356"
      },
      "source": [
        "!pip install tensorrt\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8amcT_wgVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "outputId": "cea90290-26e0-4328-9b12-97e5d0b7994e"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "2024-03-13 11:56:48.740910: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
              "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
              "2024-03-13 11:56:49.881777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
              "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
              "I0000 00:00:1710331011.394026    4488 cuda_executor.cc:1036] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
              "I0000 00:00:1710331011.451150    4488 cuda_executor.cc:1036] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
              "I0000 00:00:1710331011.451432    4488 cuda_executor.cc:1036] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
              "\n",
              "NOTE: Using experimental fast data loading logic. To disable, pass\n",
              "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
              "    https://github.com/tensorflow/tensorboard/issues/4784\n",
              "\n",
              "Traceback (most recent call last):\n",
              "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
              "    sys.exit(run_main())\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/main.py\", line 41, in run_main\n",
              "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 308, in run\n",
              "    _run_main(main, args)\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 254, in _run_main\n",
              "    sys.exit(main(argv))\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/program.py\", line 278, in main\n",
              "    return runner(self.flags) or 0\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/program.py\", line 294, in _run_serve_subcommand\n",
              "    server = self._make_server()\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/program.py\", line 469, in _make_server\n",
              "    app = application.TensorBoardWSGIApp(\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/backend/application.py\", line 139, in TensorBoardWSGIApp\n",
              "    return TensorBoardWSGI(\n",
              "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/backend/application.py\", line 252, in __init__\n",
              "    raise ValueError(\n",
              "ValueError: Duplicate plugins for name projector"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3GNLS4ywstA"
      },
      "source": [
        "## Export model inference graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rYr6VKH8_QU"
      },
      "source": [
        "The below code cell adds a line to the tf_utils.py file. This is a temporary fix to a [exporting issue](https://github.com/tensorflow/models/issues/8841) occuring when using the OD API with Tensorflow 2. This code will be removed as soon as the OD Team puts out a fix.\n",
        "\n",
        "All credit goes to Github user [Jacobsolawetz](https://github.com/Jacobsolawetz), who provided this [temporary fix](https://github.com/tensorflow/models/issues/8841#issuecomment-657647648)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usv5FMT1xG1L",
        "outputId": "8654da05-eff8-4ced-d843-c3f93d7e340e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "with open('/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py') as f:\n",
        "    tf_utils = f.read()\n",
        "\n",
        "with open('/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py', 'w') as f:\n",
        "  # Set labelmap path\n",
        "  throw_statement = \"raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\"\n",
        "  tf_utils = tf_utils.replace(throw_statement, \"if not isinstance(x, str):\" + throw_statement)\n",
        "  f.write(tf_utils)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ca333bd8b0cd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtf_utils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Set labelmap path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcvbNjcZw2er"
      },
      "source": [
        "output_directory = 'inference_graph'\n",
        "\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {model_dir} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcWVXuGAxeZ4"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(f'/content/{output_directory}/saved_model/saved_model.pb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tGVwzpLxvSv"
      },
      "source": [
        "## Test trained model on test images\n",
        "\n",
        "based on [Object Detection API Demo](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb) and [Inference from saved model tf2 colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_from_saved_model_tf2_colab.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp4wlWrhxyJL"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import six\n",
        "import time\n",
        "import glob\n",
        "from IPython.display import display\n",
        "\n",
        "from six import BytesIO\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEaYWo8WyLS2"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path (this can be local or on colossus)\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxWU7K_oyVwq"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkMddTneyesG"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.saved_model.load(f'/content/{output_directory}/saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAf3XJBzLHq"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects Elektrische Bauteile batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy()\n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "\n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEX-m3P1yp4y"
      },
      "source": [
        "import cv2\n",
        "\n",
        "for i, image_path in enumerate(glob.glob('microcontroller-detection/test/*.jpg')):\n",
        "  image = cv2.imread(image_path)\n",
        "  image_np = np.array(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  display(Image.fromarray(image_np))\n",
        "\n",
        "  if i == 9:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTG1vW9cvor0"
      },
      "source": [
        "## Convert model to Tensorflow Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vua1PJzHvsPr"
      },
      "source": [
        "output_directory = 'tf_lite'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {model_dir} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6nqRqV4v1v1"
      },
      "source": [
        "!ls -lah /content/tf_lite/saved_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3TH3JF790gk"
      },
      "source": [
        "!tflite_convert --saved_model_dir=/content/tf_lite/saved_model/ --output_file=tf_lite/model.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a3jY43W95yk"
      },
      "source": [
        "!ls -lah tf_lite/model.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jva-0AZkwdYD"
      },
      "source": [
        "## Test tflite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgeUXXFKyNtP"
      },
      "source": [
        "# based on https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/detect_picamera.py\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import re, glob\n",
        "from six import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def draw_image(image, results, size):\n",
        "    result_size = len(results)\n",
        "    for idx, obj in enumerate(results):\n",
        "        # Prepare image for drawing\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        # Prepare boundary box\n",
        "        xmin, ymin, xmax, ymax = obj['bounding_box']\n",
        "        xmin = int(xmin * size[1])\n",
        "        xmax = int(xmax * size[1])\n",
        "        ymin = int(ymin * size[0])\n",
        "        ymax = int(ymax * size[0])\n",
        "\n",
        "        # Draw rectangle to desired thickness\n",
        "        for x in range( 0, 4 ):\n",
        "            draw.rectangle((ymin, xmin, ymax, xmax), outline=(255, 255, 0), width=5)\n",
        "\n",
        "    displayImage = np.asarray( image )\n",
        "    display(Image.fromarray(displayImage))\n",
        "\n",
        "def load_labels(path):\n",
        "    \"\"\"Loads the labels file. Supports files with or without index numbers.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        labels = {}\n",
        "        for row_number, content in enumerate(lines):\n",
        "            pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\n",
        "            if len(pair) == 2 and pair[0].strip().isdigit():\n",
        "                labels[int(pair[0])] = pair[1].strip()\n",
        "            else:\n",
        "                labels[row_number] = pair[0].strip()\n",
        "    return labels\n",
        "\n",
        "def set_input_tensor(interpreter, image):\n",
        "    \"\"\"Sets the input tensor.\"\"\"\n",
        "    tensor_index = interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def get_output_tensor(interpreter, index):\n",
        "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "    output_details = interpreter.get_output_details()[index]\n",
        "    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "    \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "    set_input_tensor(interpreter, image)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get all output details\n",
        "    boxes = get_output_tensor(interpreter, 0)\n",
        "    classes = get_output_tensor(interpreter, 1)\n",
        "    scores = get_output_tensor(interpreter, 2)\n",
        "    count = int(get_output_tensor(interpreter, 3))\n",
        "\n",
        "    results = []\n",
        "    for i in range(count):\n",
        "        if scores[i] >= threshold:\n",
        "            result = {\n",
        "                'bounding_box': boxes[i],\n",
        "                'class_id': classes[i],\n",
        "                'score': scores[i]\n",
        "            }\n",
        "            results.append(result)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yrRBfVz_JwP"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"tf_lite/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "_, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGmlY2tMww27"
      },
      "source": [
        "input_mean = 0\n",
        "input_std = 1\n",
        "\n",
        "for i, image_path in enumerate(glob.glob('microcontroller-detection/test/*.jpg')):\n",
        "  image = Image.open(image_path)\n",
        "  image_pred = image.resize((input_width ,input_height), Image.ANTIALIAS)\n",
        "  if interpreter.get_input_details()[0]['dtype'] == np.float32:\n",
        "    image_pred = (np.float32(image_pred) - input_mean) / input_std\n",
        "  results = detect_objects(interpreter, image_pred, 0.5)\n",
        "\n",
        "  draw_image(image, results, image.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6-amqAb5Wwp"
      },
      "source": [
        "## Add Metadata\n",
        "from https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/convert_odt_model_to_TFLite.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NKEWMpy5Z9U"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "\n",
        "# Download the COCO dataset label map that was used to trained the SSD MobileNet V2 FPNLite 640x640 model\n",
        "!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt -q\n",
        "\n",
        "# We need to convert the Object Detection API's labelmap into what the Task API needs:\n",
        "# a txt file with one class name on each line from index 0 to N.\n",
        "# The first '0' class indicates the background.\n",
        "# This code assumes COCO detection which has 90 classes, you can write a label\n",
        "# map file for your model if re-trained.\n",
        "_ODT_LABEL_MAP_PATH = 'mscoco_label_map.pbtxt'\n",
        "_TFLITE_LABEL_PATH = \"/content/tflite_label_map.txt\"\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    _ODT_LABEL_MAP_PATH)\n",
        "f = open(_TFLITE_LABEL_PATH, 'w')\n",
        "for class_id in range(1, 91):\n",
        "  if class_id not in category_index:\n",
        "    f.write('???\\n')\n",
        "    continue\n",
        "  name = category_index[class_id]['name']\n",
        "  f.write(name+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS-x2Z8Q5bRY"
      },
      "source": [
        "from tflite_support.metadata_writers import object_detector\n",
        "from tflite_support.metadata_writers import writer_utils\n",
        "\n",
        "_TFLITE_MODEL_WITH_METADATA_PATH = '/content/model_with_metadata.tflite'\n",
        "\n",
        "writer = object_detector.MetadataWriter.create_for_inference(\n",
        "    writer_utils.load_file('tf_lite/model.tflite'), input_norm_mean=[127.5],\n",
        "    input_norm_std=[127.5], label_file_paths=[_TFLITE_LABEL_PATH])\n",
        "writer_utils.save_file(writer.populate(), _TFLITE_MODEL_WITH_METADATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfgqyX3K5hLZ"
      },
      "source": [
        "from tflite_support import metadata\n",
        "\n",
        "displayer = metadata.MetadataDisplayer.with_model_file(_TFLITE_MODEL_WITH_METADATA_PATH)\n",
        "print(\"Metadata populated:\")\n",
        "print(displayer.get_metadata_json())\n",
        "print(\"=============================\")\n",
        "print(\"Associated file(s) populated:\")\n",
        "print(displayer.get_packed_associated_file_list())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}