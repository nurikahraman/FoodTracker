{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurikahraman/FoodTracker/blob/main/%F0%9F%8D%95_Food_Recognition_Challenge_Data_Exploration_%26_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cc_H7z-N0o"
      },
      "source": [
        "# <center> üçï Food Recognition Challenge : Data Exploration & Baseline </center>\n",
        "\n",
        "<center>So, in this project, we are building a Deep Learning Model which is capable to detect various food using Detectron2, coco, along with other libraries such as Weights & Biases for recording our experimentations, and a bunch of other things</center>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "# Problem\n",
        "Detecting & Segmenting various kinds of food from an image. For ex. Someone got into new restaurent and get a food that he has never seen, well our DL model is in rescue, so our DL model will help indentifying which food it is from the class our model is being trained on!    \n",
        "\n",
        "![](https://i.imgur.com/zS2Nbf0.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "# Data\n",
        "We will be using data from Food Recognition Challenge - A benchmark for image-based food recognition challange which was launched on March 9, 2020 and ended on May 26, 2020.\n",
        "\n",
        "\n",
        "https://www.aicrowd.com/challenges/food-recognition-challenge#datasets\n",
        "\n",
        "- We have a total of **24120 RGB images** with **2053 validation**, all in **MS-COCO format** and test set for now is same as validation ( debug mode ).\n",
        "\n",
        "<br>\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "The evaluation metrics is IOU aka. Intersection Over Union ( more about that later ).\n",
        "\n",
        "The actualy metric is computed by averaging over all the precision and recall values for IOU which greater than 0.5.\n",
        "\n",
        "https://www.aicrowd.com/challenges/food-recognition-challenge#evaluation-criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxiQ8X2MFFhI"
      },
      "source": [
        "# Table of Content\n",
        "\n",
        "1. Setting our Workspace üíº\n",
        "  - Downloading & Unzipping our Dataset\n",
        "  - Downloading & Importing Necessary Libraries\n",
        "\n",
        "\n",
        "2. Data Exploration üßê\n",
        "\n",
        "  - Reading our Dataset\n",
        "  - Data Visualisations\n",
        "\n",
        "3. Image Visulisation üñºÔ∏è\n",
        "   - Reading Images\n",
        "\n",
        "4. Creating our Dataset üî®\n",
        "  - Fixing the Dataset\n",
        "  - Creating our dataset\n",
        "\n",
        "5. Creating our Model üè≠\n",
        "   - Creating R-CNN Model\n",
        "   - Setting up hyperparameters\n",
        "   \n",
        "6. Training the Model üöÇ\n",
        "  - Setting up Tensorboard\n",
        "  - Start Training!\n",
        "\n",
        "7. Evaluating the model üß™\n",
        "  - Evaluating our Model\n",
        "\n",
        "7. Testing the Model üíØ\n",
        "  - Testing the Model\n",
        "\n",
        "8. Submitting our predictions üìù\n",
        "\n",
        "9. Generate More Data + Some tips & tricks üí°\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxKD3Sok3rCK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwDa7_wkVQhK"
      },
      "source": [
        "# Setting our Workspace üíº\n",
        "\n",
        "In this section we will be downloading our dataset, unzipping it & downliading detectron2 library and importing all libraries that we will be using"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrKWgJsdVh7T"
      },
      "source": [
        "## Downloading & Unzipping our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_bUc949JVgxE"
      },
      "outputs": [],
      "source": [
        "# Downloading Training Dataset\n",
        "#!wget -q https://datasets.aicrowd.com/default/aicrowd-public-datasets/food-recognition-challenge/v0.4/train-v0.4.tar.gz -O train.tar.gz\n",
        "\n",
        "# Downloading Validation Dataset\n",
        "!wget -q https://datasets.aicrowd.com/default/aicrowd-public-datasets/food-recognition-challenge/v0.4/val-v0.4.tar.gz -O val.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RJ1nB0XTV7Y2",
        "outputId": "133ebbfe-a3d9-4e62-adef-4818af6d753e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of train.zip or\n",
            "        train.zip.zip, and cannot find train.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# Unzipping Training Dataset\n",
        "!unzip train.zip > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Yat_fhYLgZkc",
        "outputId": "643fd6a5-ada8-4e6e-b91b-97e542d07f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: not in gzip format\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "# Unzipping Validation Dataset\n",
        "!tar -xvzf val.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pPSEFV4gryz"
      },
      "source": [
        "So, the data structure is something like this\n",
        "\n",
        "```\n",
        "content\n",
        "|\n",
        "‚îî‚îÄ‚îÄ‚îÄ sample_data\n",
        "|\n",
        "‚îî‚îÄ‚îÄ‚îÄ Train\n",
        "‚îÇ   ‚îÇ   annotations.json\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄimages\n",
        "‚îÇ       ‚îÇ   012170.jpg\n",
        "‚îÇ       ‚îÇ   012030.jpg\n",
        "‚îÇ       ‚îÇ   ...\n",
        "‚îÇ   \n",
        "‚îî‚îÄ‚îÄ‚îÄ val\n",
        "‚îÇ   ‚îÇ   annotations.json\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄimages\n",
        "‚îÇ       ‚îÇ   011397.jpg\n",
        "‚îÇ       ‚îÇ   012340.jpg\n",
        "‚îÇ       ‚îÇ   ...\n",
        "|    train.zip\n",
        "|    val.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5CR0muClo9k"
      },
      "source": [
        "## Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yPa_aQL_14d",
        "outputId": "059e9fd0-0dc2-4f79-adfa-2701ff489e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 13 13:30:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Making sure that we are using GPUs\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8OTClx3ltgP",
        "outputId": "bb63069f-0e35-4b79-ae3f-021c36a334ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.9)\n",
            "Requirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "Collecting git+https://****@github.com/nurikahraman/FoodTracker.git#subdirectory=PythonAPI\n",
            "  Cloning https://****@github.com/nurikahraman/FoodTracker.git to /tmp/pip-req-build-7cl50tsq\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/nurikahraman/FoodTracker.git' /tmp/pip-req-build-7cl50tsq\n",
            "  Resolved https://****@github.com/nurikahraman/FoodTracker.git to commit ed8e98f2d8d3b0dd0daa787f681619fa10c412d1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.7) (67.7.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.7) (3.0.9)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.7) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0.7) (1.16.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.7-cp310-cp310-linux_x86_64.whl size=375665 sha256=1ba658502a54afb1f6645cfb6459062df7f591860d9a0afe61c3709e0177e5f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vq4jd5nd/wheels/30/33/96/4fc67d0bfaae36d897d368225c7a2b2d987ad5b4c84b69ce82\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "Successfully installed pycocotools-2.0.7\n",
            "2.1.0+cu121 True\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==2.1.0 torchvision==0.16.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install cython pyyaml==6.0.1\n",
        "!pip install -U 'git+https://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@github.com/nurikahraman/FoodTracker.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAlXY2UAl28H",
        "outputId": "cd8d906c-da41-46ef-b23f-0ff47f2804fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install detectron2:\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fHOXIyRql2-l"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "# For reading annotations file\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# utilities\n",
        "from pprint import pprint # For beautiful print!\n",
        "import os\n",
        "\n",
        "# For data visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E16Z8mB747v_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mYpIFBwGpjc"
      },
      "source": [
        "# Data Exploration üßê\n",
        "\n",
        "In this section we are going to read our dataset & doing some data visualisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ0LiLXFI9k4"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "K5QSFOoV8x8H",
        "outputId": "d181b467-efe2-4eab-8cc3-8957f6cea028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: /content/val.zip: unknown suffix -- ignored\n",
            "gzip: /content/train.zip: unknown suffix -- ignored\n",
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/aicrowd/annotations/training.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-288c82420132>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mVAL_ANNOTATIONS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/val/annotations.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_coco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_ANNOTATIONS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done (t={:0.2f}s)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/aicrowd/annotations/training.json'"
          ]
        }
      ],
      "source": [
        "#!echo \"Google Drive deposu baglaniyor...\"\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "!gunzip \"/content/val.zip\"\n",
        "!gunzip  \"/content/train.zip\"\n",
        "\n",
        "# Reading annotations.json\n",
        "\n",
        "TRAIN_ANNOTATIONS_PATH = \"/content/drive/MyDrive/aicrowd/annotations/training.json\"\n",
        "TRAIN_IMAGE_DIRECTIORY = \"/content/train/images/\"\n",
        "\n",
        "VAL_IMAGE_DIRECTIORY = \"/content/drive/MyDrive/aicrowd/annotations/validation.json\"\n",
        "VAL_ANNOTATIONS_PATH = \"/content/val/annotations.json\"\n",
        "\n",
        "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwzbU8zaKUw3"
      },
      "outputs": [],
      "source": [
        "# Reading the annotation files\n",
        "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
        "  train_annotations_data = json.load(f)\n",
        "\n",
        "with open(VAL_ANNOTATIONS_PATH) as f:\n",
        "  val_annotations_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0frH88WSK48l"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_annotations_data['annotations'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REa66aaYKKXE"
      },
      "source": [
        "Data Format:\n",
        "\n",
        "Our COCO data format is something like this -\n",
        "\n",
        "```\n",
        "\"info\": {...},\n",
        "\"categories\": [...],\n",
        "\"images\": [...],\n",
        "\"annotations\": [...],\n",
        "```\n",
        "\n",
        "In which categories is like this\n",
        "```\n",
        "[\n",
        "  {'id': 2578,\n",
        "  'name': 'water',\n",
        "  'name_readable': 'Water',\n",
        "  'supercategory': 'food'},\n",
        "  {'id': 1157,\n",
        "  'name': 'pear',\n",
        "  'name_readable': 'Pear',\n",
        "  'supercategory': 'food'},\n",
        "  ...\n",
        "  {'id': 1190,\n",
        "  'name': 'peach',\n",
        "  'name_readable': 'Peach',\n",
        "  'supercategory': 'food'}\n",
        "]\n",
        "```\n",
        "\n",
        "Info is empty ( not sure why )\n",
        "\n",
        "images is like this\n",
        "\n",
        "```\n",
        "[\n",
        "  {'file_name': '065537.jpg',\n",
        "  'height': 464,\n",
        "  'id': 65537,\n",
        "  'width': 464},\n",
        "  {'file_name': '065539.jpg',\n",
        "  'height': 464,\n",
        "  'id': 65539,\n",
        "  'width': 464},\n",
        " ...\n",
        "  {'file_name': '069900.jpg',\n",
        "  'height': 391,\n",
        "  'id': 69900,\n",
        "  'width': 392},\n",
        "]\n",
        "```\n",
        "Annotations is like this\n",
        "\n",
        "```\n",
        "{'area': 44320.0,\n",
        " 'bbox': [86.5, 127.49999999999999, 286.0, 170.0],\n",
        " 'category_id': 2578,\n",
        " 'id': 102434,\n",
        " 'image_id': 65537,\n",
        " 'iscrowd': 0,\n",
        " 'segmentation': [[235.99999999999997,\n",
        "   372.5,\n",
        "   169.0,\n",
        "   372.5,\n",
        "   ...\n",
        "   368.5,\n",
        "   264.0,\n",
        "   371.5]]}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zupMavO4t-LJ"
      },
      "outputs": [],
      "source": [
        "# Reading all classes\n",
        "\n",
        "category_ids = train_coco.loadCats(train_coco.getCatIds())\n",
        "\n",
        "category_names = [_[\"name_readable\"] for _ in category_ids]\n",
        "\n",
        "pprint(\", \".join(category_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1Xd6H02xeYg"
      },
      "outputs": [],
      "source": [
        "category_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPRemybjvYQt"
      },
      "outputs": [],
      "source": [
        "# Getting all categoriy with respective to their total images\n",
        "\n",
        "no_images_per_category = {}\n",
        "\n",
        "for n, i in enumerate(train_coco.getCatIds()):\n",
        "  imgIds = train_coco.getImgIds(catIds=i)\n",
        "  label = category_names[n]\n",
        "  no_images_per_category[label] = len(imgIds)\n",
        "\n",
        "img_info = pd.DataFrame(train_coco.loadImgs(train_coco.getImgIds()))\n",
        "\n",
        "no_images_per_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRLi--5Hg_Pk"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(no_images_per_category.items()).sort_values(1).iloc[::-1][0][:30].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20dpYYZqJAEt"
      },
      "source": [
        "## Data Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGvZzBTmy0vN"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure([go.Bar(x=list(no_images_per_category.keys()), y=list(no_images_per_category.values()))])\n",
        "fig.update_layout(\n",
        "    title=\"No of Image per class\",)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shRFBMIX6M3_"
      },
      "outputs": [],
      "source": [
        "pprint(f\"Average number of image per class : { sum(list(no_images_per_category.values())) / len(list(no_images_per_category.values())) }\")\n",
        "pprint(f\"Highest number of image per class is : { list(no_images_per_category.keys())[0]} of { list(no_images_per_category.values())[0] }\")\n",
        "pprint(f\"Lowest number of image per class is : Veggie Burger of { sorted(list(no_images_per_category.values()))[0] }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqPpfHFV1BZd"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Pie(labels=list(no_images_per_category.keys()), values=list(no_images_per_category.values()),\n",
        "                             hole=.3, textposition='inside', )], )\n",
        "fig.update_layout(\n",
        "    title=\"No of Image per class ( In pie )\",)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21mxnhm-5mnv"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=img_info['height']))\n",
        "fig.add_trace(go.Histogram(x=img_info['width']))\n",
        "\n",
        "# Overlay both histograms\n",
        "fig.update_layout(barmode='stack', title=\"Histogram of Image width & height\",)\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7RUZ8zNXxL"
      },
      "source": [
        "# Image Visulisation üñºÔ∏è\n",
        "\n",
        "In this section  we are going to do imaghe visualisations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB-6WMABPNn1"
      },
      "outputs": [],
      "source": [
        "img_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_od0Qvcy2WL"
      },
      "outputs": [],
      "source": [
        "len(train_annotations_data['annotations'][n]['segmentation']), len(train_annotations_data['annotations'][n]['bbox'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PnqCf3pxp8D"
      },
      "outputs": [],
      "source": [
        "for n, i in enumerate(tqdm((train_annotations_data['annotations']))):\n",
        "\n",
        "  # if np.array(train_annotations_data['annotations'][n]['segmentation']).shape[0] != np.array(train_annotations_data['annotations'][n]['bbox']).shape[0]:\n",
        "\n",
        "    # print(n)\n",
        "  if np.array(train_annotations_data['annotations'][n]['segmentation']).shape[0] != 1:\n",
        "    print(n)\n",
        "\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7I-tSAlHohS"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_no = 4\n",
        "\n",
        "annIds = train_coco.getAnnIds(imgIds=train_annotations_data['annotations'][img_no]['image_id'])\n",
        "anns = train_coco.loadAnns(annIds)\n",
        "\n",
        "# load and render the image\n",
        "\n",
        "plt.imshow(plt.imread(TRAIN_IMAGE_DIRECTIORY+train_annotations_data['images'][img_no]['file_name']))\n",
        "plt.axis('off')\n",
        "# Render annotations on top of the image\n",
        "train_coco.showAnns(anns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxZmzxXVQF31"
      },
      "outputs": [],
      "source": [
        "w, h = 12, 12 # Setting width and height of every image\n",
        "rows, cols = 5, 5 # Setting the number of image rows & cols\n",
        "\n",
        "fig = plt.figure(figsize=(20, 14)) # Making the figure with size\n",
        "\n",
        "plt.title(\"Images\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Going thought every cell in rows and cols\n",
        "for i in range(1, cols * rows+1):\n",
        "\n",
        "  annIds = train_coco.getAnnIds(imgIds=img_info['id'][i])\n",
        "  anns = train_coco.loadAnns(annIds)\n",
        "\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "\n",
        "  # Show the image\n",
        "\n",
        "  img = plt.imread(TRAIN_IMAGE_DIRECTIORY+img_info['file_name'][i])\n",
        "\n",
        "  for i in anns:\n",
        "    [x,y,w,h] = i['bbox']\n",
        "    cv2.rectangle(img, (int(x), int(y)), (int(x+h), int(y+w)), (255,0,0), 5)\n",
        "\n",
        "  plt.imshow(img)\n",
        "\n",
        "  # Render annotations on top of the image\n",
        "  train_coco.showAnns(anns)\n",
        "\n",
        "\n",
        "  # Setting the axis off\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "# Showing the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjXs1F3taBMH"
      },
      "source": [
        "# Creating our Dataset üî®\n",
        "\n",
        "In this section we are goind to fix out dataset first ( because there is some issues with dataset ( size mismatch ) & creating our dataset to put into the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-rglUXYaB8g"
      },
      "source": [
        "## Fixing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThQhfzeEyjnC"
      },
      "outputs": [],
      "source": [
        "np.array(train_annotations_data['annotations'][n]['segmentation']).shape , np.array(train_annotations_data['annotations'][n]['bbox']).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvdTrwPvY0U1"
      },
      "outputs": [],
      "source": [
        "# Function for taking a annotation & directiory of images and returning new annoation json with fixed image size info\n",
        "def fix_data(annotations, directiory):\n",
        "  for n, i in enumerate(tqdm((annotations['images']))):\n",
        "\n",
        "      img = cv2.imread(directiory+i[\"file_name\"])\n",
        "\n",
        "\n",
        "      if img.shape[0] != i['height']:\n",
        "          annotations['images'][n]['height'] = img.shape[0]\n",
        "          print(i[\"file_name\"])\n",
        "          print(annotations['images'][n], img.shape)\n",
        "\n",
        "      if img.shape[1] != i['width']:\n",
        "          annotations['images'][n]['width'] = img.shape[1]\n",
        "          print(i[\"file_name\"])\n",
        "          print(annotations['images'][n], img.shape)\n",
        "\n",
        "  return annotations\n",
        "\n",
        "\n",
        "train_annotations_data = fix_data(train_annotations_data, TRAIN_IMAGE_DIRECTIORY)\n",
        "\n",
        "\n",
        "with open('/content/train/new_ann.json', 'w') as f:\n",
        "    json.dump(train_annotations_data, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rC87-bPqKZ4"
      },
      "outputs": [],
      "source": [
        "def fix_data_val(annotations, directiory):\n",
        "  for n, i in enumerate(tqdm((annotations['images']))):\n",
        "\n",
        "      img = cv2.imread(directiory+i[\"file_name\"])\n",
        "\n",
        "\n",
        "\n",
        "      if img.shape[0] != i['height']:\n",
        "          print(n)\n",
        "          annotations['images'][n]['height'] = img.shape[0]\n",
        "          print(i[\"file_name\"])\n",
        "          print(annotations['images'][n], img.shape)\n",
        "\n",
        "      if img.shape[1] != i['width']:\n",
        "          annotations['images'][n]['width'] = img.shape[1]\n",
        "          print(i[\"file_name\"])\n",
        "          print(annotations['images'][n], img.shape)\n",
        "\n",
        "  return annotations\n",
        "\n",
        "val_annotations_data = fix_data_val(val_annotations_data, VAL_IMAGE_DIRECTIORY)\n",
        "\n",
        "with open('/content/val/new_ann.json', 'w') as f:\n",
        "    json.dump(val_annotations_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZyNHp-630wr"
      },
      "outputs": [],
      "source": [
        "dict_addres = val_annotations_data['images'][748]\n",
        "plt.imread(VAL_IMAGE_DIRECTIORY+dict_addres['file_name']).shape[:2], (dict_addres['height'], dict_addres['width'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXg_nG3P2qor"
      },
      "outputs": [],
      "source": [
        "for n, i in enumerate(val_annotations_data['images']):\n",
        "  if i['file_name'] == '053879.jpg':\n",
        "    print(n)\n",
        "    print(\"yes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnCqpxmjdx1o"
      },
      "source": [
        "## Creating our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnoBR_t6exwJ"
      },
      "outputs": [],
      "source": [
        "train_annotations_path = '/content/train/new_ann.json'\n",
        "train_images_path = '/content/train/images'\n",
        "\n",
        "val_annotations_path = '/content/val/new_ann.json'\n",
        "val_images_path = '/content/val/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh91J5MGaU7Q"
      },
      "outputs": [],
      "source": [
        "register_coco_instances(\"training_dataset\", {},train_annotations_path, train_images_path)\n",
        "\n",
        "register_coco_instances(\"validation_dataset\", {},val_annotations_path, VAL_IMAGE_DIRECTIORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVrHnP_BfMeY"
      },
      "source": [
        "# Creating our Model üè≠\n",
        "\n",
        "We are going to make an Faster R-CNN Model using Detectron2, and setting up hyperpamaters to train our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa89ntbpfRU0"
      },
      "source": [
        "## Creating R-CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t658Pm0xe2AI"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "# Check the model zoo and use any of the models ( from detectron2 github repo)\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"training_dataset\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "# Loading pre trained weights\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TOQaXMDfWo4"
      },
      "source": [
        "## Setting up hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7f8Ng25fWxR"
      },
      "outputs": [],
      "source": [
        "# No. of Batchs\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "\n",
        "# Learning Rate:\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "\n",
        "# No of Interations\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "\n",
        "# Images per batch (Batch Size)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "\n",
        "# No of Categories(Classes) present\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 273\n",
        "\n",
        "cfg.OUTPUT_DIR = \"/content/logs/\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN_VI7Z3e6Ag"
      },
      "outputs": [],
      "source": [
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xstkfNrfe16"
      },
      "source": [
        "# Training the Model üöÇ\n",
        "\n",
        "Setting up Tensorboard & finally training our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0JyMidf68L"
      },
      "source": [
        "## Setting up Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zodv5cuaE-fm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+git://github.com/wandb/client.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EEj_x7cEhxl"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project='food detection', sync_tensorboard=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a1sJjJBe8_t"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3EMCrzlf9wK"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f0PPXe2gNAF"
      },
      "source": [
        "## Start Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRA1kK2EgNWH"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYTQzkmhDRZ"
      },
      "source": [
        "# Evaluating the model üß™\n",
        "\n",
        "After training is done, we will evaluate our model to see how it's performing in the unseen images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr20Bx0lhHQF"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.01\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "evaluator = COCOEvaluator(\"validation_dataset\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"validation_dataset\")\n",
        "valResults = inference_on_dataset(trainer.model, val_loader, evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7i7k93AhEMJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.01\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 273\n",
        "\n",
        "cfg.DATASETS.TEST = (\"validation_dataset\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnMoGL7VUOEo"
      },
      "outputs": [],
      "source": [
        "val_metadata = MetadataCatalog.get(\"val_dataset\")\n",
        "\n",
        "im = cv2.imread(\"val/images/084035.jpg\")\n",
        "\n",
        "outputs = predictor(im)\n",
        "\n",
        "v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=val_metadata,\n",
        "                   scale=2,\n",
        "\n",
        "\n",
        "                   instance_mode=ColorMode.IMAGE_BW\n",
        "    )\n",
        "\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwEPtpxvKOcL"
      },
      "outputs": [],
      "source": [
        "trainer.resume_or_load(resume=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ5T9p9XBWdb"
      },
      "source": [
        "# Making Submission on AICrowd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTtj3n0RrcjH"
      },
      "source": [
        "Step 0 : Fork the baseline to make your own changes to it. Go to settings and make the repo private.\n",
        "\n",
        "\n",
        "Step 1 : Setting up SSH to login locally to Gitlab\n",
        "\n",
        "  1. Run `ssh-keygen -t ecdsa -b 521`\n",
        "  2. Run `cat ~./ssh/id_ecdsa.pub` and copy the output\n",
        "  3. Go to [Gitlab SSH Keys](https://gitlab.aicrowd.com/profile/keys) and then paste the output inside the key and use whaever title you like.\n",
        "\n",
        "Step 2: Clone Repo & Add Models & Push Changes\n",
        "\n",
        "  1. Run `git clone git@gitlab.aicrowd.com:[Your Username]/food_recognition_detectron2_baseline.git`\n",
        "  2. Put your model inside the data directioary and then run `git-lfs track \"data/\"`\n",
        "  3. Run `git stage .` then `git commit -m \" adding model\"`\n",
        "  3. Run `git push origin master`\n",
        "\n",
        "Step 3. Create Submission\n",
        "\n",
        "  1. Go to the repo and then tags and then New Tag.\n",
        "  2. In the tag name, use `submission-v1`, ( Everytime you make a new submission, just increase the no. like - `submission-v2`,  `submission-v3` )\n",
        "  3. A new issue will be created with showing the process. Enjoy!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you do not have SSH Keys, Check this link\n",
        "\n",
        "Add your SSH Keys to your GitLab account by following the instructions here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTyJxDJfjl9q"
      },
      "outputs": [],
      "source": [
        "aicrowd_submission = {\n",
        "    \"author\": \"Shubham\",\n",
        "    \"username\": \"Shubhamai\",\n",
        "    \"description\": \"detectron2 trial 2\",\n",
        "    \"model_path\": \"logs/model_final.pth\",\n",
        "    \"model_type\": \"model_zoo\",\n",
        "    \"model_config_file\": \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\",\n",
        "    \"detectron_model_config\": {\n",
        "      \"ROI_HEADS\": {\n",
        "        \"SCORE_THRESH_TEST\": 0.3,\n",
        "        \"NUM_CLASSES\": 273\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "aicrowd_submission[\"description\"] = aicrowd_submission[\"description\"].replace(\" \", \"-\")\n",
        "with open(\"aicrowd.json\", \"w\") as fp:\n",
        "  json.dump(aicrowd_submission, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PKKwhlkKCfc"
      },
      "source": [
        "## Submit to AIcrowd\n",
        "\n",
        "**Note:** We will create an SSH key on your google drive. This key will be used to identify you on gitlab.aicrowd.com."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjv1i8a92_Is"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "\n",
        "coco_api = COCO(TRAIN_ANNOTATIONS_PATH)\n",
        "\n",
        "category_ids = sorted(coco_api.getCatIds())\n",
        "categories = coco_api.loadCats(category_ids)\n",
        "\n",
        "class_to_category = { int(class_id): int(category_id) for class_id, category_id in enumerate(category_ids) }\n",
        "\n",
        "with open(\"class_to_category.json\", \"w\") as fp:\n",
        "  json.dump(class_to_category, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFB2X_nkrGU5"
      },
      "outputs": [],
      "source": [
        "!bash <(curl -sL https://gitlab.aicrowd.com/jyotish/food-recognition-challenge-detectron2-baseline/raw/master/utils/submit-colab.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJptawT0AttR"
      },
      "source": [
        "# Generate More Data + Some tips & tricks üí°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6XJFSLjDTzv"
      },
      "source": [
        "[CLoDSA](https://github.com/joheras/CLoDSA) seems a really great choice to generate more dataset. Fortunately there is also a really cool colab notebook on how to generate more dataset using CLoDSA\n",
        "\n",
        "---\n",
        "\n",
        "There is also an ongoing youtube series series from Immersive Limit on how to generate more dataset using blender ! [Blender for AI Devs](https://www.youtube.com/watch?v=UtRxC9HPGd0)\n",
        "\n",
        "[![Blender for AI Devs](https://img.youtube.com/vi/UtRxC9HPGd0/0.jpg)](https://www.youtube.com/watch?v=UtRxC9HPGd0)\n",
        "\n",
        "And also this --\n",
        "\n",
        "\n",
        "\n",
        "[![Automated Background Switching in Blender](https://img.youtube.com/vi/wuFfd7Ndrxw/0.jpg)](https://www.youtube.com/watch?v=wuFfd7Ndrxw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLTS8ywQ61f2"
      },
      "source": [
        "# Data Argumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZrd_y-U7tra"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GUfFH9CC-V6"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.core import *\n",
        "from fastai.vision.utils import *\n",
        "from fastai.vision.augment import *\n",
        "from fastai.data.core import *\n",
        "from fastai.data.transforms import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXDtk-9d7F56"
      },
      "outputs": [],
      "source": [
        "images, lbl_bbox = get_annotations('/content/train/annotations.json')\n",
        "# idx=2\n",
        "# coco_fn,bbox = coco/'train'/images[idx],lbl_bbox[idx]\n",
        "\n",
        "# def _coco_bb(x):  return TensorBBox.create(bbox[0])\n",
        "# def _coco_lbl(x): return bbox[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaCAHqhq8efV"
      },
      "outputs": [],
      "source": [
        "idx=4\n",
        "coco_fn,bbox = '/content/train/images/'+images[idx],lbl_bbox[idx]\n",
        "\n",
        "def _coco_bb(x):  return TensorBBox.create(bbox[0])\n",
        "def _coco_lbl(x): return bbox[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puSDhj5L8fbW"
      },
      "outputs": [],
      "source": [
        "coco_dsrc = Datasets([coco_fn]*10, [PILImage.create, [_coco_bb,], [_coco_lbl, MultiCategorize(add_na=True)]], n_inp=1)\n",
        "coco_tdl = TfmdDL(coco_dsrc, bs=9, after_item=[BBoxLabeler(), PointScaler(), ToTensor(), Resize(256)],\n",
        "                  after_batch=[IntToFloatTensor(), *aug_transforms()])\n",
        "\n",
        "coco_tdl.show_batch(max_n=9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "üçï Food Recognition Challenge : Data Exploration & Baseline.ipynb",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}